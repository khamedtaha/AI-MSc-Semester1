{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; background-color: RGB(190,22,220);\" >\n",
    "<h1 style=\"margin: auto; padding: 30px; \">Wikipedia Data Scraping and Exploratory Data Analysis (EDA) Mini-Project  </h1>\n",
    "</div\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">  Initialization</h3>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Brief introduction to Games and Toys as a concept.\n",
    "Importance of analyzing this category (e.g., its role in culture, development, or entertainment).\n",
    "Objectives of the project:\n",
    "Scrape data from Wikipedia.\n",
    "Structure the data into analyzable formats.\n",
    "Perform exploratory data analysis to derive insights.\n",
    "Tools and libraries used (e.g., Python, BeautifulSoup, pandas, matplotlib, etc.).\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">  Importing Libraries</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9UPmxrGdEuP",
    "outputId": "7cea2828-465a-43ce-b542-546e24996315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\anaconda\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from wikipedia-api) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->wikipedia-api) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->wikipedia-api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->wikipedia-api) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "\n",
    "!pip install wikipedia-api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHfSIDcjev6x",
    "outputId": "f7301244-33d4-4b71-a64a-208e5f45ca75"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \">  2. Dataset Scraping </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "\n",
    "# # Initialize Wikipedia API with a valid User-Agent\n",
    "# USER_AGENT = \"salaheddine/1.0 (jawadkhenfer@gmail.com)\"\n",
    "# wiki_wiki = wikipediaapi.Wikipedia('en', headers={\"User-Agent\": USER_AGENT})\n",
    "\n",
    "# # Helper Functions\n",
    "# def calculate_text_complexity(text):\n",
    "#     complex_words = len([word for word in text.split() if len(word) > 6])\n",
    "#     avg_word_length = np.mean([len(word) for word in text.split()])\n",
    "#     return round(complex_words * avg_word_length, 2)\n",
    "\n",
    "# def calculate_readability(text):\n",
    "#     words = text.split()\n",
    "#     avg_sentence_length = len(words) / max(len(re.findall(r'\\w+[.!?]', text)), 1)\n",
    "#     syllable_count = sum(1 for word in words if len(word) > 2)\n",
    "#     return round(206.835 - (1.015 * avg_sentence_length) - (84.6 * syllable_count / len(words)), 2)\n",
    "\n",
    "# def extract_key_topics(text, top_n=5):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', max_features=top_n)\n",
    "#     tfidf_matrix = vectorizer.fit_transform([text])\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     return ', '.join(feature_names)\n",
    "\n",
    "# def detect_age_group(text):\n",
    "#     age_indicators = {\n",
    "#         'children': ['kid', 'child', 'young', 'elementary'],\n",
    "#         'teen': ['teen', 'teenage', 'adolescent'],\n",
    "#         'adult': ['adult', 'professional', 'complex']\n",
    "#     }\n",
    "#     text_lower = text.lower()\n",
    "#     for group, keywords in age_indicators.items():\n",
    "#         if any(keyword in text_lower for keyword in keywords):\n",
    "#             return group\n",
    "#     return 'unspecified'\n",
    "# def download_wikipedia_image(page_title):\n",
    "\n",
    "#     # Construct Wikipedia page URL\n",
    "#     url = f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\"\n",
    "\n",
    "#     try:\n",
    "#         # Fetch the page content\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#         # Find the first image element in the infobox or main content\n",
    "#         img = soup.find('table', class_='infobox').find('img') if soup.find('table', class_='infobox') else soup.find('img')\n",
    "\n",
    "#         if img:\n",
    "#             # Get the image source\n",
    "#             img_src = img.get('src')\n",
    "\n",
    "#             # Ensure it's a full URL\n",
    "#             if img_src and img_src.startswith('//'):\n",
    "#                 img_src = 'https:' + img_src\n",
    "\n",
    "#             # Return the image URL if valid\n",
    "#             if img_src.startswith('https'):\n",
    "#                 return img_src\n",
    "\n",
    "#         return None  # No valid image found\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"HTTP error: {e}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing images for {page_title}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def extract_advanced_features(page):\n",
    "#     summary = page.summary\n",
    "\n",
    "#     # Get image URLs using the full page URL\n",
    "#     image_urls = download_wikipedia_image(page.title)\n",
    "\n",
    "#     return {\n",
    "#         'name': page.title,\n",
    "#         'summary': summary,\n",
    "#         'url': page.fullurl,\n",
    "#         'image_urls': image_urls,\n",
    "#         'text_length': len(summary),\n",
    "#         'word_count': len(summary.split()),\n",
    "#         'sentences_count': len(re.findall(r'\\w+[.!?]', summary)),\n",
    "#         'complexity_score': calculate_text_complexity(summary),\n",
    "#         'readability_score': calculate_readability(summary),\n",
    "#         'topic_keywords': extract_key_topics(summary),\n",
    "#         'age_group_hint': detect_age_group(summary)\n",
    "#     }\n",
    "# def scrape_and_analyze(search_terms, max_depth=1):\n",
    "\n",
    "#     scraped_data = []\n",
    "#     visited_pages = set()\n",
    "\n",
    "#     def scrape_page(page, depth):\n",
    "#         if depth > max_depth or page.title in visited_pages or not page.exists():\n",
    "#             return\n",
    "#         visited_pages.add(page.title)\n",
    "#         try:\n",
    "#             features = extract_advanced_features(page)\n",
    "#             scraped_data.append(features)\n",
    "#             # Recursively scrape linked pages\n",
    "#             for link_title in page.links.keys():\n",
    "#                 link_page = wiki_wiki.page(link_title)\n",
    "#                 scrape_page(link_page, depth + 1)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error scraping page {page.title}: {e}\")\n",
    "\n",
    "#     for term in search_terms:\n",
    "#         start_page = wiki_wiki.page(term)\n",
    "#         scrape_page(start_page, 0)\n",
    "\n",
    "#     return pd.DataFrame(scraped_data)\n",
    "\n",
    "# # Perform EDA\n",
    "# def perform_advanced_eda(df):\n",
    "#     plt.figure(figsize=(20, 15))\n",
    "\n",
    "#     # 1. Complexity Score Distribution\n",
    "#     plt.subplot(2, 3, 1)\n",
    "#     sns.histplot(df['complexity_score'], kde=True)\n",
    "#     plt.title('Text Complexity Distribution')\n",
    "\n",
    "#     # 2. Readability Score\n",
    "#     plt.subplot(2, 3, 2)\n",
    "#     sns.boxplot(x=df['readability_score'])\n",
    "#     plt.title('Readability Scores')\n",
    "\n",
    "#     # 3. Age Group Distribution\n",
    "#     plt.subplot(2, 3, 3)\n",
    "#     df['age_group_hint'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "#     plt.title('Age Group Distribution')\n",
    "\n",
    "#     # 4. Correlation Heatmap\n",
    "#     plt.subplot(2, 3, 4)\n",
    "#     correlation_matrix = df[['text_length', 'word_count', 'complexity_score', 'readability_score']].corr()\n",
    "#     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "#     plt.title('Feature Correlations')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"\\nKey Insights:\")\n",
    "#     print(f\"Total Unique Topics: {df['topic_keywords'].nunique()}\")\n",
    "#     print(f\"Average Complexity Score: {df['complexity_score'].mean():.2f}\")\n",
    "#     print(f\"Average Readability Score: {df['readability_score'].mean():.2f}\")\n",
    "\n",
    "# # Save the dataset\n",
    "# def save_dataset(df, filename='advanced_toys_dataset.csv'):\n",
    "#     df.to_csv(filename, index=False)\n",
    "#     print(f\"Dataset saved: {filename}\")\n",
    "\n",
    "# # Main Execution\n",
    "# search_terms = [\n",
    "#     'Toys', 'Board Games', 'Video Games', 'Educational Toys', 'Classic Games',\n",
    "#     'Children Toys', 'Puzzle Games', 'Vintage Toys', 'Action Figures',\n",
    "#     'Outdoor Toys', 'Card Games', 'Creative Toys', 'Electronic Games',\n",
    "#     'STEM Toys', 'Role-Playing Games', 'Family Games' ,\n",
    "# ]\n",
    "\n",
    "# toys_df = scrape_and_analyze(search_terms, max_depth=1)\n",
    "\n",
    "# # Perform EDA\n",
    "# # perform_advanced_eda(toys_df)\n",
    "\n",
    "# # Save the dataset\n",
    "# save_dataset(toys_df)\n",
    "\n",
    "# # Show the first few rows of the dataset\n",
    "# toys_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid RGB(51,165,182);\" >\n",
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(51,165,182); \"> 3. Data Structuring </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
